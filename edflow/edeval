#!/usr/bin/env python3

import argparse
from edflow.eval.pipeline import standalone_eval_csv_file, cbargs2cbdict
from edflow.eval.pipeline import config2cbdict
from edflow.config import parse_unknown_args, update_config

A = argparse.ArgumentParser(
    description="""
Use edeval for running callbacks on data generated using the
``edflow.eval_pipeline.eval_pipeline.EvalHook``. Once the data is created all
you have to do is pass the ``csv``-file created by the hook. It specifies all
the relevant information: Which dataset was used to create the data, along with
all config parameters and where all generated samples live.

Callbacks will be evaluated in the order they have been passed to this
function. They must be supplied in `name:callback` pairs.

For more documentation take a look at ``edflow/eval_pipeline/eval_pipeline.py``
"""
)

A.add_argument(
    "-c",
    "--csv",
    default="model_output.csv",
    type=str,
    help="path to a csv-file created by the EvalHook containing"
    " samples generated by a model.",
)

A.add_argument(
    "-cb",
    "--callback",
    type=str,
    nargs="*",
    help="Import string to the callback functions used for the "
    "standalone evaluation. Callbacks must be supplied as name:import_path"
    "values.",
)

A.add_argument(
    "-cf",
    "--config",
    type=str,
    help="Path to a config containing a dict of callbacks. Callbacks with the "
    "same name as those supplied via the commandline will be overwritten by "
    "the commandline callbacks (-cb).",
    default=None,
)

args, unknown = A.parse_known_args()
additional_kwargs = parse_unknown_args(unknown)

config = dict()
if args.config is not None:
    config = yaml.full_load(args.config)

update_config(config, additional_kwargs)

callbacks, cb_kwargs = config2cbdict(config)
commandline_callbacks = cbargs2cbdict(args.callback)
callbacks.update(commandline_callbacks)

standalone_eval_csv_file(args.csv, callbacks, additional_kwargs, cb_kwargs)
